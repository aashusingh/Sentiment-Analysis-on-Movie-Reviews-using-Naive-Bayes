{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Set 1a: Text classification using Naive Bayes\n",
    "==============\n",
    "\n",
    "\n",
    "\n",
    "In this problem set, we will build a system for classifying movie reviews as positive, negative, or neutral. You will:\n",
    "\n",
    "- Do some basic text processing, tokenizing your input and converting it into a bag-of-words representation\n",
    "- Build a classifier based on sentiment word lists\n",
    "- Build a machine learning classifier, using Naive Bayes\n",
    "- Evaluate your classifiers and examine what they have learned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation ##\n",
    "\n",
    "You may need to install some of the libraries below. Usually this is done with pip or easy_install. See here:\n",
    "\n",
    "- [NLTK](http://www.nltk.org/install.html)\n",
    "- [matplotlib](http://matplotlib.org/users/installing.html)\n",
    "- [numpy](http://docs.scipy.org/doc/numpy/user/install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import scorer\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing #\n",
    "(_Completing docsToBOWs() - 4 pts, each question in Deliverable 1 is worth 1 pt. Total 8 pts for part 1_)\n",
    "\n",
    "Your first step is to write code that can apply the following\n",
    "preprocessing steps. You will have to run this code fairly quickly on\n",
    "the test data when you receive it, so make sure it is modular and\n",
    "well-written.\n",
    "\n",
    "- You will edit a function that takes as its argument a \"key\" document.\n",
    "  It should produce a \"BOW\" (bag-of-words) document.\n",
    "  Each line of the key document contains a filename and a label.\n",
    "  Each line of the BOW document should contain a BOW representation of the corresponding\n",
    "  file in the key document. \n",
    "- A BOW representation looks like this: \"word:count word:count word:count...\" for every word that appears in\n",
    "  the document. Do not print words that have zero count. Use space delimiters.\n",
    "- Use NLTK's [tokenization package](http://nltk.org/api/nltk.tokenize.html) function \n",
    "  to divide each file into sentences, and each sentence into tokens.\n",
    "- Downcase all tokens\n",
    "- Only consider tokens that are completely alphabetic.\n",
    "\n",
    "I have provided some shell code, but you will have to fill in the tokenization and filtering steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def docsToBOWs(keyfile):\n",
    "    with open(keyfile,'r') as keys:\n",
    "        with open(keyfile.replace('.key','.bow'),'w') as outfile:\n",
    "            for keyline in keys:\n",
    "                dataloc = keyline.rstrip().split(' ')[0]\n",
    "                fcounts = dict()\n",
    "                with open(dataloc,'r') as infile:\n",
    "                    for line in infile: \n",
    "                        l = line.decode('ascii', 'ignore')\n",
    "                        token_sent = sent_tokenize(l)\n",
    "                        for s in token_sent:\n",
    "                            token_word = word_tokenize(s)\n",
    "                            for w in token_word:\n",
    "                                if w.isalpha():\n",
    "                                    w = w.lower()\n",
    "                                    if fcounts.has_key(w):\n",
    "                                        fcounts[w] += 1\n",
    "                                    else:\n",
    "                                        fcounts[w] = 1\n",
    "                for word,count in fcounts.items():\n",
    "                    print >>outfile,\"{}:{}\".format(word,count), #write the word and its count to a line\n",
    "                print >>outfile,\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the keyfiles that are relevant to this homework. \n",
    "At the beginning you won't have test-imdb.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainkey = 'train-imdb.key'\n",
    "devkey = 'dev-imdb.key'\n",
    "testkey = 'test-imdb.key'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run these lines to produce the BOW files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docsToBOWs(trainkey)\n",
    "docsToBOWs(devkey)\n",
    "# docsToBOWs('test-imdb.key') # you won't have this file yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines a [generator function](http://wiki.python.org/moin/Generators), called \"dataIterator\"\n",
    "\n",
    "- This allows you to easily iterate through the dataset defined by a given keyfile. \n",
    "- Each time you call \"next\" (possibly implicitly), it returns a dict containing features and counts for the next document in the sequence. \n",
    "- In this case, the features include the words, and a special \"offset\" feature\n",
    "- This is equivalent to $\\boldsymbol{x}_i$ in the reading.\n",
    "- You can see how this is used in the getAllCounts() function below, which takes a dataIterator as an argument.\n",
    "\n",
    "Lines 7-8 of the code might look confusing if you are not a pythonista. \n",
    "\n",
    "- This is a [list comprehension](http://legacy.python.org/dev/peps/pep-0202/)\n",
    "nested inside a [dict comprehension](http://legacy.python.org/dev/peps/pep-0274/).\n",
    "- Here's an [introduction](http://carlgroner.me/Python/2011/11/09/An-Introduction-to-List-Comprehensions-in-Python.html) with more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offset = '**OFFSET**'\n",
    "def dataIterator(keyfile):\n",
    "    with open(keyfile.replace('key','bow'),'r') as bows:\n",
    "        with open(keyfile,'r') as keys:\n",
    "            for keyline in keys:\n",
    "                textloc,label = keyline.rstrip().split(' ')\n",
    "                fcounts = {word:int(count) for word,count in\\\n",
    "                           [x.split(':') for x in bows.readline().rstrip().split(' ')]}\n",
    "                fcounts[offset] = 1\n",
    "                yield fcounts,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataIterator above incrementally re-reads the keyfile and BOW file every time you call it. \n",
    "This is a good idea if you have huge data that won't fit in memory, but the file I/O involves some overhead.\n",
    "If you want, you can write a second dataIterator that iterates across data stored in memory, which\n",
    "will be faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check**: How many unique words appear in the training set? (Types, not tokens.) I get 24861. (Don't count the offset feature.)\n",
    "\n",
    "- Note how the dataIterator function is used here. \n",
    "- fcounts is a dict, which it returns for each document.\n",
    "- We are currently ignoring the label, but that is also provided.\n",
    "- This may take a couple of minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word types in training set:  24999\n",
      "Number of tokens in training set:  466683\n",
      "Number of word types in dev set:  16965\n",
      "Number of tokens in dev set:  223994\n",
      "Token to type ratio for training set:  18.6680667227\n",
      "Token to type ratio for dev set:  13.2033009136\n",
      "Words which appear in dev data but not in training data:  4843\n",
      "Number of words which exactly once in training data:  10910\n",
      "Number of words which exactly once in dev data:  8064\n"
     ]
    }
   ],
   "source": [
    "def getAllCounts(datait):\n",
    "    allcounts = Counter()\n",
    "    for fcounts, _ in datait:\n",
    "        allcounts += Counter(fcounts)\n",
    "    return allcounts\n",
    "\n",
    "ac_train = getAllCounts(dataIterator('train-imdb.key'))\n",
    "ac_dev = getAllCounts(dataIterator('dev-imdb.key'))\n",
    "print \"Number of word types in training set: \",len(ac_train.keys())-1\n",
    "print \"Number of tokens in training set: \",sum(ac_train.values())-1\n",
    "print \"Number of word types in dev set: \",len(ac_dev.keys())-1\n",
    "print \"Number of tokens in dev set: \",sum(ac_dev.values())-1\n",
    "print \"Token to type ratio for training set: \", (sum(ac_train.values())-1) / (len(ac_train.keys())-1) \n",
    "print \"Token to type ratio for dev set: \", (sum(ac_dev.values())-1) / (len(ac_dev.keys())-1) \n",
    "print \"Words which appear in dev data but not in training data: \", len(set(ac_dev.keys()) - set(ac_train.keys()))\n",
    "print \"Number of words which exactly once in training data: \", len([item for item in ac_train.values() if item==1])\n",
    "print \"Number of words which exactly once in dev data: \", len([item for item in ac_dev.values() if item==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code makes a plot, with the log-rank (from 1 to the log of the total number of words) \n",
    "on the x-axis and the log count on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# this enables you to create inline plots in the notebook \n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x70fadf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEPCAYAAABLIROyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2e32430>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_logcounts = np.log(np.array(sorted(ac_train.values(),reverse=True)))\n",
    "plt.plot(np.log(range(len(tr_logcounts))),tr_logcounts)\n",
    "dv_logcounts = np.log(np.array(sorted(ac_dev.values(),reverse=True)))\n",
    "plt.plot(np.log(range(len(dv_logcounts))),dv_logcounts,'r')\n",
    "plt.xlabel('log rank')\n",
    "plt.ylabel('log count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 1**\n",
    "\n",
    "- Explain what you see in the plot. Does it observe Zipf's law? How well?\n",
    "- Print the token/type ratio for both the training and dev data.\n",
    "- Print the number of types which appear exactly once in the training and dev data\n",
    "- Print the number of types that appear in the dev data but not the training data (hint: use [sets](https://docs.python.org/2/library/sets.html) for this)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a) Above graphs are linear with negative slope. \n",
    "   Generalized equation of for both lines : log count = K - log rank where K is a constant.\n",
    "   Taking exponential on both sides : count = {e^k} / {rank}, which is what Zipf's law states. For boundary cases , like low rank or low count the graph loses its linearity property.\n",
    "\n",
    "b) Number of word types in training set:  24999\n",
    "   Number of tokens in training set:  466683\n",
    "   Number of word types in dev set:  16965 \n",
    "   Number of tokens in dev set:  223994\n",
    "   Token to type ratio for training set:  18.6680667227\n",
    "   Token to type ratio for dev set:  13.2033009136\n",
    "\n",
    "c) Number of words which exactly once in training data:  10910\n",
    "   Number of words which exactly once in dev data:  8064\n",
    "\n",
    "d) Words which appear in dev data but not in training data:  4843"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Lists #\n",
    "(_Completing predict() - 3 pts, setting weights - 2 pts, Deliverable 2 - 1 pt. Total 7 pts for part 2_)\n",
    "\n",
    "- We will now build a sentiment analysis system based on word lists. \n",
    "- The file \"sentiment-vocab.tff\" contains a sentiment lexicon from [ Wilson et al 2005](http://people.cs.pitt.edu/~wiebe/pubs/papers/emnlp05polarity.pdf). \n",
    "- The code below reads the lexicon into memory, building sets of positive and negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "poswords = set()\n",
    "negwords = set()\n",
    "with open('sentiment-vocab.tff','r') as fin:\n",
    "    for i,line in enumerate(fin):\n",
    "        # more list and dict comprehensions!\n",
    "        kvs = {key:val for key,val in [kvp.split('=') for kvp in line.split() if '=' in kvp]}\n",
    "        if kvs['type'] == 'strongsubj':\n",
    "            if kvs['priorpolarity'] == 'negative':\n",
    "                negwords.add(kvs['word1'])\n",
    "            if kvs['priorpolarity'] == 'positive':\n",
    "                poswords.add(kvs['word1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you should write a classifier that classifies each instance in a testfile. The classification rule is:\n",
    "\n",
    "- 'POS' if the instance has more words from the positive list than the negative list\n",
    "- 'NEG' if the instance has more words from the negative list than the positive list\n",
    "- 'NEU' (neutral) if the instance has the same number of words from each list\n",
    "\n",
    "To do this, you will write a function \"predict\", \n",
    "which represents the inner-product computation $\\boldsymbol{\\theta}' \\boldsymbol{f}(\\boldsymbol{x},y)$.\n",
    "It should have the following characteristics:\n",
    "\n",
    "- **Input 1** an instance, represented as a dict (with features as keys and counts as values) \n",
    "- **Input 2** a dictionary of weights, where keys are tuples of features and labels, and weights are the values. This corresponds to $\\boldsymbol{\\theta}$ in the reading. See example below.\n",
    "- **Input 3** a list of possible labels\n",
    "- **Output 1** the highest-scoring label\n",
    "- **Output 2** a dict with labels as keys and scores as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use this to find the highest-scoring label\n",
    "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(instance,weights,labels):\n",
    "    scores = defaultdict(int)\n",
    "    for w , count in instance.items():\n",
    "        for label in labels:\n",
    "            scores[label] = scores.get(label,0) + count*weights.get((label,w),0)\n",
    "    return argmax(scores),scores\n",
    "    # return the highest-scoring label, and the scores for all labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are weights for the simplest classifier, which simply labels all instances as positive.\n",
    "\n",
    "Note that it uses only the 'offset' feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_labels = ['POS','NEG','NEU']\n",
    "weights_all_pos = defaultdict(int)\n",
    "weights_all_pos.update({('POS',offset):1,('NEG',offset):0,('NEU',offset):0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code for evaluating your classifiers. \n",
    "It uses a scoring library that I wrote, and writes the output to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evalClassifier(weights,outfilename,testfile=devkey):    \n",
    "    with open(outfilename,'w') as outfile: #open the output file\n",
    "        for counts,label in dataIterator(testfile): #iterate through eval set\n",
    "            print >>outfile, predict(counts,weights,all_labels)[0] #print prediction to file\n",
    "    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shows how to evaluate this classifier. \n",
    "\n",
    "- **Sanity check**: You should get 40.7% accuracy just by classifying everything as positive. This is the \"most common class\" (MCC) baseline.\n",
    "\n",
    "The printed output is a **confusion matrix**. \n",
    "The rows indicate the key and the columns indicate the response. \n",
    "In this case, the response is always \"POS\", so there is only one column. \n",
    "The cell NEG/POS tells you how often an example that was labeled \"NEG\" in the key was labeled \"POS\" in the system response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
      "1 classes in response: set(['POS'])\n",
      "confusion matrix\n",
      "key\\response:\tPOS\n",
      "NEG\t\t401\t\n",
      "NEU\t\t192\t\n",
      "POS\t\t407\t\n",
      "----------------\n",
      "accuracy: 0.4070 = 407/1000\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mat = evalClassifier(weights_all_pos,'all_pos.txt')\n",
    "print scorer.printScoreMessage(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a classifier based on the word lists. The classifier should have the following decision rule:\n",
    "\n",
    "- If the number of positive words (tokens) is greater than the number of negative words, choose label 'POS'\n",
    "- If the number of negative words (tokens) is greater than the number of positive words, choose label 'NEG'\n",
    "- If they are equal, choose label 'NEU'\n",
    "\n",
    "You should manually set the weights to ensure this behavior; don't change the predict function at all.\n",
    "You'll need to use the offset weights to make sure that ties go to the 'NEU' label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights_list = defaultdict(int)\n",
    "for item in poswords:\n",
    "    weights_list.update({('POS',item): 3})\n",
    "for item in negwords:\n",
    "    weights_list.update({('NEG',item): 3})\n",
    "weights_list.update({('NEU',offset):  9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 2**: run your classifier on dev.key, and use the following code to print the resulting confusion matrix.\n",
    "\n",
    "The confusion matrix should now have three columns, since the response should include every class at least once. The count of correct responses is found on the diagonal of the confusion matrix. What is the most frequent type of error?\n",
    "\n",
    "**Sanity check**: The accuracy should be 55.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
      "3 classes in response: set(['NEG', 'NEU', 'POS'])\n",
      "confusion matrix\n",
      "key\\response:\tNEG\tNEU\tPOS\n",
      "NEG\t\t220\t17\t164\t\n",
      "NEU\t\t61\t17\t114\t\n",
      "POS\t\t51\t36\t320\t\n",
      "----------------\n",
      "accuracy: 0.5570 = 557/1000\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "mat = evalClassifier(weights_list,'word_list.txt')\n",
    "print scorer.printScoreMessage(mat)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Most common type of error is the number of 'negative' tagged words which have been classified as positive. This value comes out to be 164.This might be due to the fact that these words occur more frequently in the files labeled positive in the dev dataset.\n",
    "Both positive and negative have been assigned equal weights and offset given a large value to resolve the tie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Naive Bayes #\n",
    "(_Completing learnNBWeights() - 5 pts, Deliverable 3a - 1pt, 3b - 1 pt, explanation of plot output - 2pts. Total 8 points for part 3_)\n",
    "\n",
    "Now you will implement a Naive Bayes classifier.\n",
    "\n",
    "You already have the code for the decision function, \"predict\". \n",
    "So you just need to construct a set of weights that correspond to the classifier. \n",
    "These weights will contain two parameters:\n",
    "\n",
    "- $\\log \\mu$ for the offset, which parametrizes the prior $\\log P(y)$\n",
    "- $\\log \\phi$ for the word counts, which parametrizes the likelihood $\\log P(x | y)$\n",
    "\n",
    "You should use maximum *a posteriori* estimation of\n",
    "the parameter $\\phi$,\n",
    "$$\\phi_{j,n} = P(w = n | y = j) = \\frac{\\sum_{i: y_i = j} x_{i,n} + \\alpha}{\\sum_{i:y_i=j} \\sum_{n'} x_{i,n'} + V\\alpha}$$\n",
    "where \n",
    "\n",
    "- $y_i = j$ indicates the class label $j$ for instance $i$\n",
    "- $w=n$ indicates word $n$\n",
    "- $\\alpha$ is the smoothing parameter\n",
    "- $V$ is the total number of words\n",
    "\n",
    "For each class, normalize by the sum of counts of words **in that class**. In other words, $\\sum_j \\phi_{j,n} = 1$ for all $j$. You can estimate $\\log \\phi$ directly if you prefer.\n",
    "\n",
    "For the prior $\\log P(y)$, you can use relative frequency estimation.\n",
    "\n",
    "Both probabilities should be estimated from the training data only.\n",
    "Please write this code yourself -- do not use other libraries, and try to do\n",
    "it without looking at other code online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain #hint, especially if you're obsessive about being pythonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the word counts first, because this can be slow\n",
    "# you may also wish to keep a list of all word types that are observed in the training data\n",
    "counts = defaultdict(lambda : Counter()) # hint\n",
    "class_counts = defaultdict(int) # hint\n",
    "\n",
    "def getCounts_all(datait):\n",
    "    pos_counts = Counter()\n",
    "    neg_counts = Counter()\n",
    "    neu_counts = Counter()\n",
    "    global neg_docCount,pos_docCount, neu_docCount\n",
    "    neg_docCount = 0 \n",
    "    pos_docCount = 0\n",
    "    neu_docCount = 0\n",
    "    \n",
    "    for fcounts, label in datait:\n",
    "        if label == 'POS':\n",
    "            pos_counts += Counter(fcounts)\n",
    "            pos_docCount += 1\n",
    "        elif label == 'NEG':\n",
    "            neg_counts += Counter(fcounts)\n",
    "            neg_docCount += 1\n",
    "        elif label == 'NEU':\n",
    "            neu_counts += Counter(fcounts)\n",
    "            neu_docCount += 1\n",
    "    return pos_counts,neg_counts,neu_counts\n",
    "\n",
    "pos_train,neg_train,neu_train  = getCounts_all(dataIterator('train-imdb.key'))\n",
    "\n",
    "pos_sum = sum(pos_train.values())\n",
    "neg_sum = sum(neg_train.values())\n",
    "neu_sum = sum(neu_train.values())\n",
    "\n",
    "total_words = len(ac_train.keys())-1\n",
    "total_docCount  = pos_docCount + neg_docCount + neu_docCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should write a *function* to compute the weights for a given value of $\\alpha$, \n",
    "because you will want to vary this value later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def learnNBWeights(alpha=0.1):\n",
    "    weights =  defaultdict(float)# your code here\n",
    "    bayes_numerator = float()\n",
    "    bayes_denominator = float()\n",
    "    log_phi = float()\n",
    "    \n",
    "    for word in pos_train:\n",
    "        bayes_numerator = pos_train[word] + alpha\n",
    "        bayes_denominator = pos_sum + total_words*alpha\n",
    "        log_phi = np.log(bayes_numerator) - np.log(bayes_denominator)\n",
    "        weights.update({('POS',word): log_phi})\n",
    "        \n",
    "    for word in neg_train:\n",
    "        bayes_numerator = neg_train[word] + alpha\n",
    "        bayes_denominator = neg_sum + total_words*alpha\n",
    "        log_phi = np.log(bayes_numerator) - np.log(bayes_denominator)\n",
    "        weights.update({('NEG',word): log_phi})\n",
    "        \n",
    "    for word in neu_train:\n",
    "        bayes_numerator = neu_train[word] + alpha\n",
    "        bayes_denominator = neu_sum + total_words*alpha\n",
    "        log_phi = np.log(bayes_numerator) - np.log(bayes_denominator)\n",
    "        weights.update({('NEU',word): log_phi})\n",
    "    \n",
    "    weights.update({('POS',offset): (np.log(pos_docCount)- np.log(total_docCount))})\n",
    "    weights.update({('NEG',offset): (np.log(neg_docCount)- np.log(total_docCount))})\n",
    "    weights.update({('NEU',offset): (np.log(neu_docCount)- np.log(total_docCount))})  \n",
    "    return weights\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3a**\n",
    "Train a classifier from the training data, and apply it to\n",
    "the development data, with $\\alpha = 0.1$. Report the confusion matrix and the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the weights\n",
    "weights_nb = learnNBWeights(alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check**: For the following instance, I get the scores shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('NEG',\n",
       " defaultdict(<type 'int'>, {'NEG': -34.774652009636632, 'NEU': -40.685465798167527, 'POS': -44.324484194528054}))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict({'good':1,'worst':4,offset:1},weights_nb,all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 classes in key: set(['NEG', 'NEU', 'POS'])\n",
      "3 classes in response: set(['NEG', 'NEU', 'POS'])\n",
      "confusion matrix\n",
      "key\\response:\tNEG\tNEU\tPOS\n",
      "NEG\t\t101\t268\t32\t\n",
      "NEU\t\t40\t140\t12\t\n",
      "POS\t\t111\t264\t32\t\n",
      "----------------\n",
      "accuracy: 0.2730 = 273/1000\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# run this code to evaluate your weights\n",
    "mat = evalClassifier(weights_nb,'nb.txt')\n",
    "print scorer.printScoreMessage(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 3b** Try at least seven different values of $\\alpha$. Plot the accuracy on both the dev and training sets for each value, using [subplot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.subplot) to show two plots in the same cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.273\n",
      "0.5 0.288\n",
      "0.9 0.286\n",
      "1 0.291\n",
      "4 0.338\n",
      "9 0.347\n",
      "18 0.329\n",
      "69 0.305\n",
      "100 0.3\n",
      "1000 0.26\n"
     ]
    }
   ],
   "source": [
    "tr_accs = []\n",
    "dv_accs = []\n",
    "alphas = [0.1,0.5,0.9,1,4,9,18,69,100,1000]\n",
    "weights_nb_alphas = dict()\n",
    "for alpha in alphas:\n",
    "    print alpha,\n",
    "    # learn the weights\n",
    "    weights_nb_alphas[alpha] = learnNBWeights(alpha) \n",
    "    # evaluate on training data\n",
    "    confusion = evalClassifier(weights_nb_alphas[alpha],'nb.alpha.tr.txt',trainkey)\n",
    "    tr_accs.append(scorer.accuracy(confusion))\n",
    "    # evaluate on dev data\n",
    "    confusion = evalClassifier(weights_nb_alphas[alpha],'nb.alpha.dv.txt',devkey)\n",
    "    dv_accs.append(scorer.accuracy(confusion))\n",
    "    print dv_accs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x9e42d90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEACAYAAACkvpHUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYrFUiP8HdmdnMQvziOsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7014a70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run this code to plot the accuracies\n",
    "subplot(1,2,1)\n",
    "plot(log(alphas),tr_accs,'bx-')\n",
    "ylabel('training accuracy')\n",
    "subplot(1,2,2)\n",
    "plot(log(alphas),dv_accs,'rx-')\n",
    "ylabel('dev. accuracy')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "With negligible values of smoothing parameter(or absence of it), for any new word, the naive bayes classifier gives a zero probability without any consideration for prior liklihood. Adding the smoothing parameter helps it learn to classify new encountered words correctly. With fine balance of smoothing parameter, naive bayes classifier performs well - on both training and unseen dev data. \n",
    "With large values of smoothing parameter, it becomes the dominating factor and classifies most of the words in a single class irrespective of the evidence.\n",
    "Also it represents, a bias variance tradeoff where training accuracy reaches only a maximum limit since its a linear classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Analysis #\n",
    "\n",
    "(_Completing  getTopFeats() - 2 pts, Deliverable 4a - 1pt, 4b - 2 pts, 4c - 2 pts, 4d -5pts . Total 7 pts for CS4650 and 12 pts for CS7650_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4a**\n",
    "What are the words that are most predictive of positive versus negative text?\n",
    "You can measure this by $\\log \\theta_{pos,n} - \\log \\theta_{neg,n}$ (which is similar to the [likelihood ratio test](http://en.wikipedia.org/wiki/Likelihood-ratio_test)).\n",
    "Use $\\alpha = 1$ from the dev data.\n",
    "\n",
    "List the top five words and their counts for each class. Do the same for the top 5 words that predict negative versus positive.\n",
    "\n",
    "Consider using [operator.itemgetter()](http://docs.python.org/2.7/library/operator.html) for easily sorting dictionaries by their values. See my definition of the argmax function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getTopFeats(weights,class1,class2,K=5):\n",
    "    class1_Dict = defaultdict(float)\n",
    "    class2_Dict = defaultdict(float)\n",
    "    class_Ratio = defaultdict(float)\n",
    "    for word,value in weights.iteritems():\n",
    "        if word[0] == class1:\n",
    "            class1_Dict[word[1]] = value\n",
    "        elif word[0] == class2:\n",
    "            class2_Dict[word[1]] = value\n",
    "    intersect = set(class1_Dict.keys()).intersection(set(class2_Dict.keys()))\n",
    "    for word in intersect:\n",
    "        class_Ratio[word] = class1_Dict[word] - class2_Dict[word]\n",
    "    sorted_list = sorted(class_Ratio.iteritems(), key=operator.itemgetter(1), reverse = True)\n",
    "    count_list = [ (k[0],( pos_train[k[0]],  neg_train[k[0]],  neu_train[k[0]])) for k in sorted_list[:K]]\n",
    "    return count_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following format is used: (word,(Positive_wordCount, Negative_wordCount, Neutral_wordCount))\n",
      "[('subtle', (30, 1, 5)), ('emotions', (26, 1, 9)), ('themes', (26, 1, 8)), ('homer', (26, 1, 0)), ('victor', (19, 1, 4))]\n",
      "[('awful', (4, 103, 19)), ('redeeming', (1, 26, 1)), ('lance', (1, 24, 0)), ('hulk', (1, 23, 0)), ('waste', (7, 82, 8))]\n"
     ]
    }
   ],
   "source": [
    "# run this\n",
    "print \"Following format is used: (word,(Positive_wordCount, Negative_wordCount, Neutral_wordCount))\"\n",
    "print getTopFeats(weights_nb_alphas[1],'POS','NEG')\n",
    "print getTopFeats(weights_nb_alphas[1],'NEG','POS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4b** Now do the same thing for $\\alpha = 100$. Which words look better to you? \n",
    "Which gave better accuracy? \n",
    "Explain what you think is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following format is used: (word,(Positive_wordCount, Negative_wordCount, Neutral_wordCount))\n",
      "[('great', (457, 133, 164)), ('best', (311, 107, 85)), ('excellent', (114, 19, 27)), ('wonderful', (100, 13, 15)), ('love', (284, 123, 71))]\n",
      "[('bad', (127, 496, 130)), ('worst', (17, 169, 25)), ('awful', (4, 103, 19)), ('worse', (17, 111, 15)), ('waste', (7, 82, 8))]\n"
     ]
    }
   ],
   "source": [
    "# run this\n",
    "print \"Following format is used: (word,(Positive_wordCount, Negative_wordCount, Neutral_wordCount))\"\n",
    "print getTopFeats(weights_nb_alphas[100],'POS','NEG')\n",
    "print getTopFeats(weights_nb_alphas[100],'NEG','POS')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "When smoothing parameter has a negligible value or is 0,the naive bayes classifier gives a zero probability for newly encountered words without any consideration for prior likelihood. Adding the smoothing parameter helps it learn to classify new encountered words correctly.This helps in improving accuracy.\n",
    "For alpha = 100, even though the naive bayes classifier performs well compared to when alpha = 1, its not the best performance as can be seen from the graph. The classifier reaches peak accuracy around 9 or 10. \n",
    "Results for alpha = 9: More strong words like 'greatest' can be observed.\n",
    "\n",
    "[('wonderful', (100, 13, 15)), ('excellent', (114, 19, 27)), ('superb', (44, 4, 6)), ('greatest', (39, 3, 14)), ('subtle', (30, 1, 5))]\n",
    "[('awful', (4, 103, 19)), ('worst', (17, 169, 25)), ('waste', (7, 82, 8)), ('stupid', (10, 83, 23)), ('worse', (17, 111, 15))]\n",
    "\n",
    "With fine balance of smoothing parameter, naive bayes classifier performs well : on  training and unseen data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deliverable 4c** Consider the weights for $\\alpha=100$. \n",
    "\n",
    "- Print words $w$ that are in the positive lexicon, but for which $\\log \\phi_{neg,w} > \\log \\phi_{pos,w} + 0.1$. \n",
    "    - (These words are more likely in the negative class, despite being in the positive lexicon.)\n",
    "- Print words $w$ that are in the negative lexicon, but for which the $\\log \\phi_{pos,w} > \\log \\phi_{neg,w} + 0.1$. \n",
    "    - (These words are more likely in the positive class, despite being in the negative lexicon.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words which despite being in positive lexicon are classified negative: \n",
      "['compassion', 'ovation', 'delightfully', 'veritable', 'elegant', 'crave', 'valiant', 'liberty', 'feisty', 'romantically', 'joyous', 'want', 'unforgettable', 'preferably', 'preferable', 'dignified', 'astoundingly', 'devote', 'admirably', 'better', 'entranced', 'herald', 'sparkle', 'aspire', 'angelic', 'playfully', 'encouraging', 'thoughtfully', 'insightful', 'congratulatory', 'fabulous', 'enchanted', 'wholesome', 'exonerate', 'endear', 'marvelously', 'quaint', 'gutsy', 'foremost', 'relish', 'monumental', 'stunningly', 'pacifist', 'fragrant', 'sensitivity', 'humankind', 'tolerance', 'idyllic', 'exemplary', 'majestic', 'gratitude', 'privilege', 'idol', 'shimmering', 'admiration', 'delight', 'renaissance', 'groundbreaking', 'stupendous', 'just', 'zeal', 'splendor', 'revel', 'superlative', 'delighted', 'unpretentious', 'overjoyed', 'vitality', 'congratulations', 'incisive', 'soothe', 'excel', 'trepidation', 'intimacy', 'undoubted', 'befriend', 'grateful', 'devoted', 'devotee', 'amaze', 'engrossing', 'comforting', 'kindliness', 'ingenuous', 'momentous', 'gorgeously', 'breathtaking', 'vivid', 'cherished', 'indescribable', 'eloquent', 'sworn', 'paramount', 'praiseworthy', 'amuse', 'explicit', 'dreamland', 'yearn', 'solace', 'truthful', 'richly', 'giddy', 'boundless', 'sensationally', 'truthfully', 'terrifying', 'optimism', 'glossy', 'sublime', 'astonishingly', 'indelible', 'expertly', 'supremely', 'flawlessly', 'explicitly', 'infallible', 'pinnacle', 'heartwarming', 'wonder', 'steadfast', 'jovial', 'adroit', 'satisfaction', 'understated', 'blessing', 'scrupulously', 'offbeat', 'wisely', 'deft', 'cheery', 'enchanting', 'delectable', 'knowledgeable', 'lavish', 'joyful', 'grit', 'sensational', 'miraculous', 'redeeming', 'eloquence', 'zest', 'fondness', 'boldly', 'chivalry', 'tranquility', 'regard', 'curiously', 'exemplar', 'ingeniously', 'ideally', 'unparalleled', 'coherence', 'repay', 'carefree', 'candid', 'judicious', 'everlasting', 'dashing', 'whimsical', 'excellently', 'zenith', 'sacred', 'okay', 'heartfelt', 'perceptive', 'harmless', 'lovably', 'sweeping', 'respectful', 'terrify', 'glitter', 'applaud', 'salute', 'boldness', 'extraordinary', 'terrifyingly', 'obedience', 'kind', 'hilariousness', 'blissful', 'liberate', 'impressively', 'pretty', 'terrifically', 'unfettered', 'wow', 'please', 'adored', 'majesty', 'civility', 'comrades', 'credence', 'funny', 'salvation', 'inspirational', 'elegance', 'shrewdly', 'bravery', 'fabulously', 'pithy', 'manifest', 'consensus', 'entice', 'impassioned', 'fanfare', 'irresistible', 'proficient', 'exceeding', 'togetherness', 'reverence', 'radiant', 'graciously', 'graceful', 'smitten', 'phenomenally', 'destinies', 'admiring', 'suave', 'felicity', 'outdo', 'sensation', 'exuberant', 'justification', 'talent', 'meek', 'indulgence', 'magnificently', 'invincibility', 'unquestionably', 'insistent', 'justifiably', 'tact', 'honestly', 'glee', 'richness', 'incorruptible', 'handy', 'mastery', 'sentiment', 'superbly', 'honorable', 'craving', 'ethical', 'salutary', 'vouch', 'aspiration', 'devotion', 'chivalrous', 'tidy', 'skillful', 'passionately', 'hopeful', 'marvels', 'preach', 'yearning', 'complement', 'joke', 'penetrating', 'attest', 'lush', 'impeccably', 'rightfully', 'fascinate', 'scruples', 'earnestly', 'like', 'vibrant', 'liking', 'rewarding', 'humorously', 'obey', 'surge', 'articulate', 'esteem', 'doubtless', 'stirring', 'supreme']\n",
      "Words which despite being in negative lexicon are classified positive: \n",
      "['protest', 'mirage', 'offenses', 'venom', 'superficially', 'berate', 'smack', 'bewildered', 'harried', 'brutish', 'rife', 'deplorable', 'overdo', 'monotonous', 'deadbeat', 'weirdly', 'angrily', 'cannibal', 'insult', 'adamant', 'heartless', 'degenerate', 'fanaticism', 'complacent', 'scourge', 'fiendish', 'awfulness', 'grievous', 'repulsive', 'superficiality', 'misfortune', 'struggle', 'thankless', 'enraged', 'overact', 'horrendously', 'conspicuously', 'preposterous', 'rattle', 'staggeringly', 'impotent', 'digress', 'foolishly', 'ludicrous', 'degradation', 'overbearing', 'repugnant', 'disprove', 'blasted', 'obscenity', 'freakish', 'sly', 'mangle', 'pessimistic', 'stink', 'incorrectly', 'haggard', 'ragged', 'fanatically', 'puzzling', 'pitiful', 'disadvantage', 'contradict', 'stern', 'gruesomely', 'foe', 'burn', 'genocide', 'guile', 'derision', 'forgetful', 'emotional', 'radically', 'leer', 'superfluous', 'geezer', 'illogic', 'unnervingly', 'backward', 'adamantly', 'empathize', 'abhorrent', 'weakening', 'jerk', 'mourn', 'distasteful', 'irritated', 'havoc', 'drunkard', 'boast', 'stew', 'atrocious', 'nauseating', 'insultingly', 'unruly', 'drama', 'mar', 'irrational', 'scoff', 'shockingly', 'disheartening', 'careless', 'overworked', 'stench', 'incongruous', 'awfully', 'shortcomings', 'heretical', 'impractical', 'mislead', 'oppressive', 'slaves', 'yawn', 'deign', 'unravel', 'sabotage', 'imbecile', 'disgruntled', 'slashing', 'inappropriately', 'floundering', 'foreboding', 'inexplicable', 'caricature', 'concede', 'squash', 'abject', 'irrelevant', 'ineptly', 'resentful', 'underestimate', 'pedantic', 'infuriatingly', 'idiotically', 'objection', 'demean', 'stinking', 'darn', 'monstrosity', 'ulterior', 'dastardly', 'curses', 'futile', 'downer', 'gibberish', 'quandary', 'ax', 'abyss', 'disgraceful', 'midget', 'unattractive', 'pitifully', 'consternation', 'unrest', 'hideous', 'overdone', 'liars', 'misery', 'unthinkable', 'wretched', 'flak', 'blah', 'sickeningly', 'unlucky', 'gasp', 'refuse', 'spiteful', 'crippling', 'annoyingly', 'outraged', 'bereft', 'motley', 'agitated', 'fanatics', 'stodgy', 'intolerable', 'disorganized', 'vainly', 'gullible', 'bellicose', 'laughably', 'dire', 'rack', 'zealot', 'pains', 'sympathies', 'depressingly', 'miseries', 'abysmally', 'shabby', 'insufferably', 'lackadaisical', 'unreasonable', 'ugh', 'languid', 'aghast', 'discord', 'insinuating', 'naughty', 'cringe', 'bloated', 'spiritless', 'smother', 'worthless', 'filth', 'ineptitude', 'chatter', 'distressed', 'fun', 'loot', 'shoddy', 'gall', 'discredit', 'haunting', 'distracting', 'skeptical', 'mediocrity', 'insincere', 'pathetically', 'selfishness', 'wayward', 'ordeal', 'incoherence', 'obscenely', 'hysterics', 'inexcusable', 'mishandle', 'tragically', 'rejection', 'shortcoming', 'raging', 'deprive', 'nationalism', 'grind', 'inglorious', 'wrongly', 'bravado', 'mindlessly', 'implausibly', 'implausible', 'incoherent', 'excruciating', 'abominable', 'rotten', 'upsetting', 'contradiction', 'bitterly', 'disgustingly', 'fiend', 'insincerity', 'sore', 'overzealous', 'pout', 'embarrassingly', 'cliched', 'abrasive', 'furiously', 'incompetence', 'disrespecting', 'doubtful', 'tyrant', 'blatant', 'inconsistencies', 'crass', 'disregard', 'radical', 'shambles', 'implode', 'embarrass', 'galling', 'obstacle', 'keen', 'gratuitously', 'boil', 'sloppily', 'unscrupulous', 'dreary', 'egregious', 'sardonic', 'pitiable', 'embroiled', 'alarmed', 'appallingly', 'fiasco', 'maddeningly', 'madman', 'smutty', 'indignant', 'humiliation', 'catastrophic', 'lazy', 'dungeon', 'derogatory', 'tricky', 'bemoan', 'exclaim', 'insidious', 'bewildering', 'shameless', 'lecherous', 'irate', 'glaring', 'garish', 'reactionary', 'raving', 'excessive', 'morbidly', 'incessant', 'flagrant', 'stuffy', 'disbelief', 'inaccurate', 'fatalistic', 'diabolical', 'agonizing', 'heresy', 'contradictory', 'wince', 'grate', 'wretchedly', 'excruciatingly', 'treason']\n"
     ]
    }
   ],
   "source": [
    "pos_list = []\n",
    "neg_list = []\n",
    "for word in poswords:\n",
    "    if weights_nb_alphas[100].has_key(('NEG',word)) and (weights_nb_alphas[100][('NEG',word)] - weights_nb_alphas[100][('POS',word)]) > 0.1:\n",
    "        pos_list.append(word)\n",
    "for word in negwords:\n",
    "    if weights_nb_alphas[100].has_key(('POS',word)) and (weights_nb_alphas[100][('POS',word)] - weights_nb_alphas[100][('NEG',word)]) > 0.1:\n",
    "        neg_list.append(word)\n",
    "        \n",
    "print \"Words which despite being in positive lexicon are classified negative: \" \n",
    "print pos_list\n",
    "print \"Words which despite being in negative lexicon are classified positive: \" \n",
    "print neg_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for Thought: \n",
    "\n",
    "What do you think is going on here? Pick one of these words, and look for example reviews that contain it (using [grep](http://en.wikipedia.org/wiki/Grep)). \n",
    "\n",
    "Is the word used in the opposite sense, or is there some other explanation?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Consider a word which should have been in positive lexicon but is instead in the negative lexicon:Scourge\n",
    "It occurs once in train/10448_1.txt - Classification 'Negative'\n",
    "Sentence containing the word : This is the first time I've ever posted a movie review, but I felt so strongly that somebody must speak out against this scourge of cinematography.\n",
    "\n",
    "The word in this sentence is used in negative sense only.\n",
    "\n",
    "The log-probability for 'scourge' being found in the positive subset rather than negative one is,\n",
    "log(100/(Positive_worcount + vocabSize*100)) - log(101/(Negative_wordcount + vocabSize*100)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
